{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder (CAE)\n",
    "### Authors: Sergi Nogués and Gilles Salem\n",
    "\n",
    "This is the second assignment for ACML at the MSc in\n",
    "Artificial Intelligence at Maastricht University.\n",
    "\n",
    "[Source code](https://github.com/serginogues/convolutional_autoencoder)\n",
    "\n",
    "CIFAR-10 dataset: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'D:/UM/ACML/Assignments/'\n",
    "VALIDATION_SIZE = 0.1  # percentage of the training set used for validation\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "# Compatibility with CUDA and GPU -> remember to move into GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "\n",
    "**_Divide your dataset into training (80%), validation (10%) and test (10%). Normalize the data:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CIFAR10 train and test datasets can be downloaded as follows.\n",
    "Note that the _ToTensor()_ transform is applied to convert from PIL images\n",
    "to Tensors and rescale from range $[0, 255]$ to range $[0, 1]$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transforms.ToTensor())\n",
    "classes = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can plot any image to check the previous code worked:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img, label = train_dataset[6150]\n",
    "print(\"image label:\", classes[label])\n",
    "\n",
    "# since we already used the transform, type(img) = torch.Tensor\n",
    "print(img.shape)\n",
    "\n",
    "# plot with original axis before converting PIL Image to Tensor, otherwise an Exception arises\n",
    "# C × H × W to H × W × C\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to divide our dataset in test, train and validation, we need first to\n",
    "concatenate both train and test and then split with the desired proportion."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concat_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "len_ = len(concat_dataset)\n",
    "train_set, test_set, valid_set = random_split(concat_dataset, [round(len_ * TRAIN_SIZE), round(len_ * TEST_SIZE), round(len_ * VALIDATION_SIZE)])\n",
    "\n",
    "print(\"\")\n",
    "print(\"# samples train set =\", len(train_set))\n",
    "print(\"# samples test set =\", len(test_set))\n",
    "print(\"# samples validation set =\", len(valid_set))\n",
    "total_samp = len(train_set) + len(test_set) + len(valid_set)\n",
    "print(\"Sample distribution: \" + str(round((len(train_set) / total_samp) * 100))\n",
    "      + \"% train, \" + str(round((len(test_set) / total_samp) * 100)) + \"% test, \"\n",
    "      + str(round((len(valid_set) / total_samp) * 100)) + \"% validation\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=0, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "\n",
    "Xs, Ys = iter(train_loader).next()\n",
    "images = Xs.numpy()\n",
    "images = images\n",
    "plt.imshow(np.transpose(images[30], (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Implement the autoencoder network specified above. Run the training for at least 10 epochs, and plot the\n",
    "evolution of the error with epochs._**\n",
    "\n",
    "We first define our simple convolutional autoencoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "\n",
    "        padding = 1\n",
    "        stride = 1\n",
    "        kernel = 3\n",
    "        channels = [8, 12, 16, 12]\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=channels[0], kernel_size=kernel, padding=padding, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels[0], out_channels=channels[1], kernel_size=kernel, padding=padding, stride=stride)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=kernel, padding=padding, stride=stride)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel-1, stride=stride, padding=0)\n",
    "\n",
    "        # Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=channels[2], out_channels=channels[3], kernel_size=kernel+1, padding=padding, stride=stride)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=channels[3], out_channels=3, kernel_size=kernel+1, padding=padding, stride=stride)\n",
    "        self.print_latent_shape = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        if self.print_latent_shape == 0:\n",
    "            print(\"Latent space shape: \" + str(x.shape))\n",
    "            self.print_latent_shape +=1\n",
    "\n",
    "        # decoder\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        y = F.sigmoid(self.t_conv2(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then define the training hyperparameters and train the model for 10 epochs and batch size 64."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVE_PATH = 'models/cae.pth'\n",
    "SAVE_PATH2 = 'models/cae2.pth'\n",
    "SAVE_PATH3 = 'models/cae3.pth'\n",
    "SAVE_PATH4 = 'models/cae4.pth'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 0.01\n",
    "\n",
    "model = CAE().to(device)\n",
    "criterion = nn.BCELoss()  # loss function\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def train(model, save=SAVE_PATH):\n",
    "    loss_history = []\n",
    "    running_loss = 0.0\n",
    "    for i in range(EPOCHS):\n",
    "        # TRAIN MODEL\n",
    "        loss_sum = 0\n",
    "        n = 0\n",
    "        for j, data in enumerate(train_loader, 0):\n",
    "            n = j\n",
    "            # get the training data\n",
    "            images, label = data\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Before the backward pass, set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # predict\n",
    "            output = model.forward(images)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(output, images)\n",
    "            loss_sum += round(float(loss.item()), 4)\n",
    "\n",
    "            # backpropagate loss error\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize with backprop\n",
    "            optimizer.step()\n",
    "            del data, images, label\n",
    "\n",
    "        # region print current loss\n",
    "        loss_epoch = loss_sum/n\n",
    "        loss_history.append(loss_epoch)\n",
    "        print(\"Epoch \"+ str(i) +\", Loss = \"+ str(loss_epoch))\n",
    "\n",
    "        # SAVE THE MODEL every EPOCH\n",
    "        torch.save(model.state_dict(), save)\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    # PLOT ACCURACY\n",
    "    plt.plot(loss_history)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Training Loss per epoch')\n",
    "    plt.show()\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Report also the test error._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(trained_model):\n",
    "    with torch.no_grad():\n",
    "        loss_sum = 0\n",
    "        n = 0\n",
    "        for images, labels in test_loader:\n",
    "            output = trained_model.forward(images)\n",
    "            loss = criterion(output, images)\n",
    "            loss_sum += round(float(loss.item()), 4)\n",
    "            n += 1\n",
    "        loss_final = loss_sum/n\n",
    "        print(\"Test Loss = \", loss_final)\n",
    "\n",
    "cae1 = CAE()\n",
    "cae1.load_state_dict(torch.load(SAVE_PATH))\n",
    "cae1.eval()\n",
    "test(cae1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see our trained model's performance with these 5 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = iter(test_loader).next()\n",
    "images_plot = images.numpy()\n",
    "\n",
    "ff, axarr = plt.subplots(1, 5, constrained_layout=True)\n",
    "for i in range(5):\n",
    "    axarr[i].imshow(np.transpose(images_plot[i], (1, 2, 0)))\n",
    "    axarr[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 epochs of training the reconstruction is good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sample outputs\n",
    "output = cae1.forward(images)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "ff2, axarr = plt.subplots(1, 5)\n",
    "for i in range(5):\n",
    "    axarr[i].imshow(np.transpose(output[i], (1, 2, 0)))\n",
    "    axarr[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_What is the size of the latent space representation of the above network?_**\n",
    "\n",
    "$W' = \\frac{W - K + 2P}{S} + 1$\n",
    "\n",
    "Encoder:\n",
    "1. out_conv1 = batch_size x 8 x 32 x 32 where $32 = 32 - 3 + 2 + 1$\n",
    "- out_pool1 = batch_size x 8 x 31 x 31\n",
    "- out_conv2 = batch_size x 12 x 31 x 31\n",
    "- out_pool2 = batch_size x 12 x 30 x 30\n",
    "- out_conv3 = batch_size x 16 x 30 x 30\n",
    "\n",
    "Latent space size = 16 x 30 x 30 = 14400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Try other architectures (e.g. fewer intermediate layers, different number of channels, filter sizes or stride and\n",
    "padding configurations) to answer questions such as: What is the impact of those in the reconstruction error\n",
    "after training? Is there an obvious correlation between the size of the latent space representation and the error?_**\n",
    "\n",
    "We first try defining a moodel such that the latent space is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CAE2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE2, self).__init__()\n",
    "\n",
    "        channels=[5, 8, 10, 8, 5]\n",
    "        padding = 1\n",
    "        stride = 1\n",
    "        kernel = 3\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=channels[0], kernel_size=(4, 4), padding=padding, stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels[0], out_channels=channels[1], kernel_size=(4, 4), padding=padding, stride=stride)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=(4, 4), padding=padding, stride=stride)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel-1, stride=stride, padding=0)\n",
    "\n",
    "        # Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=channels[2], out_channels=channels[3], kernel_size=(6, 6), padding=padding, stride=(2, 2))\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=channels[3], out_channels=channels[4], kernel_size=(6, 6), padding=padding, stride=stride)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(in_channels=channels[4], out_channels=3, kernel_size=(6, 6), padding=padding, stride=stride)\n",
    "        self.print_latent_shape = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x2))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        if self.print_latent_shape == 0:\n",
    "            print(\"Latent space shape: \" + str(x.shape))\n",
    "            self.print_latent_shape +=1\n",
    "\n",
    "        # decoder\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        y = F.sigmoid(self.t_conv3(x))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CAE2().to(device)\n",
    "criterion = nn.BCELoss()  # loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "train(model, save=SAVE_PATH2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The latent space is now smaller than before: 10 x 12 x 12; and the training error higher."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = CAE2()\n",
    "model.load_state_dict(torch.load(SAVE_PATH2))\n",
    "model.eval()\n",
    "test(model)\n",
    "\n",
    "# Sample outputs\n",
    "output = model.forward(images)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "ff2, axarr = plt.subplots(1, 5)\n",
    "for i in range(5):\n",
    "    axarr[i].imshow(np.transpose(output[i], (1, 2, 0)))\n",
    "    axarr[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected the reconstruction test error is higher and the reconstructed images are worse.\n",
    "\n",
    "Now let's try with a more channels and more layers to improve the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CAE3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE3, self).__init__()\n",
    "\n",
    "        channels=[16, 32, 64, 32, 16]\n",
    "        padding = 1\n",
    "        stride = 1\n",
    "        kernel = 3\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=channels[0], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels[0], out_channels=channels[1], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel-1, stride=stride, padding=0)\n",
    "\n",
    "        # Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=channels[2], out_channels=channels[3], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=channels[3], out_channels=channels[4], kernel_size=(4, 4), padding=padding, stride=stride)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(in_channels=channels[4], out_channels=3, kernel_size=(4, 4), padding=padding, stride=stride)\n",
    "        self.print_latent_shape = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        if self.print_latent_shape == 0:\n",
    "            print(\"Latent space shape: \" + str(x.shape))\n",
    "            self.print_latent_shape +=1\n",
    "\n",
    "        # decoder\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        y = F.sigmoid(self.t_conv3(x))\n",
    "        return y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CAE3().to(device)\n",
    "criterion = nn.BCELoss()  # loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "train(model, save=SAVE_PATH3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The latent space is now bigger than before 64 x 30 x 30 and the training error lower."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cae3 = CAE3()\n",
    "cae3.load_state_dict(torch.load(SAVE_PATH3))\n",
    "cae3.eval()\n",
    "test(cae3)\n",
    "\n",
    "# Sample outputs\n",
    "output = cae3.forward(images)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "ff2, axarr = plt.subplots(1, 5)\n",
    "for i in range(5):\n",
    "    axarr[i].imshow(np.transpose(output[i], (1, 2, 0)))\n",
    "    axarr[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected the reconstruction test error is lower and the reconstructed images are better.\n",
    "Note that the colors are better reconstructed now."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorization\n",
    "\n",
    "**_Adapt your network from the previous part such that it learns to reconstruct colors by feeding in grayscale\n",
    "images but predicting all RGB channels. As a starting point, use the hyperparameters (including the network\n",
    "architecture) that you identified to yield the best performance in Exercise 3.2._**\n",
    "\n",
    "\n",
    "**_Report on your results and reason about potential shortcomings of your network.\n",
    "What aspects of the architecture/hyperparameters/optimization could be improved upon to fit the model more adequately to this\n",
    "application? Try out some ideas._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see what YUV channels look like, how to access Chrominance\n",
    "and how to reconstruct back the RGB color by combining the grayscale image with the two Chrominance channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "images, labels = iter(test_loader).next()\n",
    "img = images[50].numpy()\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "yuv_image = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,2)\n",
    "plt.imshow(gray_image, cmap=\"gray\")\n",
    "plt.title('Grayscale')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,3)\n",
    "plt.imshow(yuv_image[:,:,0],cmap=\"gray\")\n",
    "plt.title('Luminance')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(yuv_image[:,:,1],cmap=\"gray\")\n",
    "plt.title('Chrominance1')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(yuv_image[:,:,2],cmap=\"gray\")\n",
    "plt.title('Chrominance2')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining the 2 chrominance channels with the luminance channel we can reconstruct\n",
    "the original RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "luminance_image = yuv_image[:,:,0]\n",
    "\n",
    "chr_image = yuv_image[:,:,1:]\n",
    "yuv_reconstructed = cv2.merge((luminance_image, chr_image[:,:,0], chr_image[:,:,1]))\n",
    "\n",
    "rgb_image = cv2.cvtColor(yuv_reconstructed, cv2.COLOR_YUV2BGR)\n",
    "plt.imshow(rgb_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the goal is to predict the 2D chrominance by using as input the 1D luminance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CAE4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE4, self).__init__()\n",
    "\n",
    "        channels=[16, 32, 64, 64, 32]\n",
    "        padding = 1\n",
    "        stride = 1\n",
    "        kernel = 3\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=channels[0], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels[0], out_channels=channels[1], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel-1, stride=stride, padding=0)\n",
    "\n",
    "        # Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=channels[2], out_channels=channels[3], kernel_size=(3, 3), padding=padding, stride=stride)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=channels[3], out_channels=channels[4], kernel_size=(4, 4), padding=padding, stride=stride)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(in_channels=channels[4], out_channels=2, kernel_size=(4, 4), padding=padding, stride=stride)\n",
    "        self.print_latent_shape = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        if self.print_latent_shape == 0:\n",
    "            print(\"Latent space shape: \" + str(x.shape))\n",
    "            self.print_latent_shape +=1\n",
    "\n",
    "        # decoder\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        y = F.sigmoid(self.t_conv3(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will implement learning rate decay and early stop."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "model_colorization = CAE4().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_colorization.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 70\n",
    "def train4(model, save):\n",
    "    loss_history = []\n",
    "    running_loss = 0.0\n",
    "    for i in range(EPOCHS):\n",
    "        # TRAIN MODEL\n",
    "        loss_sum = 0\n",
    "        n = 0\n",
    "        for j, data in enumerate(train_loader, 0):\n",
    "            n = j\n",
    "            # get the training data\n",
    "            imgs, _ = data\n",
    "\n",
    "            # rgb to yuv\n",
    "            images_yuv = np.zeros((BATCH_SIZE, 32, 32, 3))\n",
    "            for idx in range(BATCH_SIZE):\n",
    "                img = imgs[idx].numpy()\n",
    "                yuv_image = cv2.cvtColor(np.transpose(img, (1, 2, 0)), cv2.COLOR_BGR2YUV)\n",
    "                images_yuv[idx] = yuv_image\n",
    "\n",
    "            # numpy to tensor\n",
    "            target = torch.tensor(np.reshape(images_yuv, (64, 3, 32, 32))[:, 1:, :, :]).float().to(device)\n",
    "            images_yuv_tensor = torch.from_numpy(np.reshape(np.reshape(images_yuv, (64, 3, 32, 32))[:, 0, :, :], (64, 1, 32, 32))).to(device)  # luminance as input\n",
    "\n",
    "            # Before the backward pass, set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # predict\n",
    "            input = images_yuv_tensor.float().to(device)\n",
    "            output = model.forward(input).to(device)  # chrominance\n",
    "            # compute loss\n",
    "            loss = criterion(output, target)\n",
    "            loss_sum += round(float(loss.item()), 4)\n",
    "\n",
    "            # backpropagate loss error\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize with backprop\n",
    "            optimizer.step()\n",
    "            del data, imgs\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # region print current loss\n",
    "        loss_epoch = loss_sum / n\n",
    "        loss_history.append(loss_epoch)\n",
    "        print(\"Epoch \" + str(i) + \", Loss = \" + str(loss_epoch))\n",
    "\n",
    "        if i > 1 and loss_epoch < loss_history[-1]:\n",
    "            # SAVE THE MODEL\n",
    "            print(\"model saved\")\n",
    "            torch.save(model.state_dict(), save)\n",
    "\n",
    "        if i>2 and loss_epoch > loss_history[-2] or i > 15 and (loss_history[1] - loss_epoch) < 0.02:\n",
    "            break\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    # PLOT ACCURACY\n",
    "    plt.plot(loss_history)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Training Loss per epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-211-e9a175ee45b8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain4\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_colorization\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSAVE_PATH4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-207-a37afc0f9fef>\u001B[0m in \u001B[0;36mtrain4\u001B[1;34m(model, save)\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0mloss_sum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m             \u001B[0mn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m             \u001B[1;31m# get the training data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    515\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    516\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 517\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    518\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    519\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    555\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    556\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 557\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    558\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    559\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    328\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    329\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 330\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    331\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    332\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    217\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    218\u001B[0m             \u001B[0msample_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0midx\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcumulative_sizes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdataset_idx\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 219\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdataset_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msample_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    220\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torchvision\\datasets\\cifar.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 120\u001B[1;33m             \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    121\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    122\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_transform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m     95\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConverted\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m         \"\"\"\n\u001B[1;32m---> 97\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     98\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     99\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.virtualenvs\\sergi_env\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001B[0m in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    136\u001B[0m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mByteTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mByteStorage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_buffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 138\u001B[1;33m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetbands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    139\u001B[0m     \u001B[1;31m# put it from HWC to CHW format\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train4(model_colorization, SAVE_PATH4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This task is much more difficult for the current model as can be seen by the loss evolution per epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, _ = iter(test_loader).next()\n",
    "images_plot = images.numpy()\n",
    "img = images[50].numpy()\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "yuv_image = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cae4 = CAE4()\n",
    "cae4.load_state_dict(torch.load(SAVE_PATH4))\n",
    "cae4.eval()\n",
    "\n",
    "luminance_image = torch.tensor(np.reshape(yuv_image[:,:,0], (1, 1, 32, 32)))\n",
    "chr_image = cae4.forward(luminance_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chr_image = np.reshape(chr_image.detach().numpy(), (32, 32, 2))\n",
    "yuv_reconstructed = cv2.merge((yuv_image[:,:,0], chr_image[:,:,0], chr_image[:,:,1]))\n",
    "reconstructed = cv2.cvtColor(yuv_reconstructed, cv2.COLOR_YUV2BGR)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(yuv_image[:,:,0],cmap=\"gray\")\n",
    "plt.title('Grayscale')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(reconstructed)\n",
    "plt.title('Colorized')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is clear that the model needs improvement. Better results could be achieved by adding more convolutional layers such that\n",
    "the encoded latent space has bigger size. This would require more computational power of course.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (convolutional_autoencoder)",
   "language": "python",
   "name": "pycharm-42d5438a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}